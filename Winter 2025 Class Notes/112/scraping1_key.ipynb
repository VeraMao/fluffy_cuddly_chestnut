{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=right>\n",
    "Winter 2025<br>\n",
    "Nardin<br>\n",
    "Lecture 2\n",
    "</div>\n",
    "\n",
    "\n",
    "<h1 align=center>Getting Data from the Web I</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkblue'> <h2>Learning Objectives</h2> </font>\n",
    "\n",
    "* Define web scraping\n",
    "* Recognize the differences between scraping with and without public APIs\n",
    "* Use the library `requests` to interact with websites\n",
    "* Use the library `BeautifulSoup` to parse and extract data from websites\n",
    "* Read and understand the HTML language\n",
    "* Select HTML tags for scraping\n",
    "\n",
    "To achive these goals, we use two examples: Chicago wikipedia page (to scrape a table) and University of Arizona's website (to scrape faculty members' information and deal with missing info)\n",
    "\n",
    "<font color='darkblue'> <h2>Table of Contents</h2> </font>\n",
    "\n",
    "1. Definition of Web Scraping and Two Ways to Scrape\n",
    "1. Scraping in Python: Overview of `requests` and `beautifulsoup`\n",
    "1. Example 1: Chicago Wikipedia\n",
    "    * Using `requests`\n",
    "    * Website Structure\n",
    "    * Using `beautifulsoup`\n",
    "    * Exercise 1\n",
    "1. Example 2: Extract Faculty Information\n",
    "    * Putting it all togheter\n",
    "    * Exercise 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkblue'> <h2>1. Definition of Web Scraping and Two Ways to Scrape</h2> </font>\n",
    " \n",
    " \n",
    "Web scraping is <b>the process of gathering or \"scraping\" information from a website.</b>\n",
    "\n",
    "If you have ever copied and pasted information from the Internet, you have performed the same task as any web scraper, just on a small scale. Web scraping allows automating this process to collect hundreds, thousands, or even millions of information (e.g., companies' names, emails, phones, newspaper articles, reviews, prices, etc.)\n",
    "\n",
    "Broadly speaking <b>there are two main ways to get data from a website</b> and we will explore both, with a greater emphasis on the second option:\n",
    "\n",
    "<font color='darkblue'> <h3> Option 1: Using the API made accessible by the website </h3> </font>\n",
    "\n",
    "Several websites, especially those of large corporations or those managing extensive datasets, offer mechanisms that allow you to gather data by submitting queries and receiving CSV or JSON files in return. A website that offers this service is said to \"provide an API.\" **An API (Application Programming Interface) is an interface provided by the website that helps users to collect data from that specific website.** \n",
    "\n",
    "How do you know if a website has an API? Most major websites (e.g. Facebook, Google, Amazon, The New York Times, etc.), usually have one or more than one (and want you to use it!). You can use Google to check if an API is available.\n",
    "\n",
    "Example: Google Books API: https://developers.google.com/books/docs/v1/using#WorkingVolumes\n",
    "\n",
    "**To gather data from a website using an API, you need to:**\n",
    "\n",
    "* Learn how the API works (every API is different: sometimes using an API is smoooth, sometimes it is a frustrating experience) and often register an account and get a password\n",
    "* You can interact with the the API directly or use Python wrapper written by someone to simplify interacting with the API; for examples see [here](https://github.com/realpython/list-of-python-api-wrappers) but use with caution is not recently updated \n",
    "\n",
    "<font color='darkblue'> <h3> Option 2: Directly accessing the website's HTML </h3> </font>\n",
    "\n",
    "Many other websites do not provide an API. To gather data from these sites, you need to scrape them directly. Note you can always **attempt** to scrape a website directly, even if it has an API. Sometimes it is easier/faster than learning howto interact with the API, but in most cases, though, it is better to use the API, if one is available (examine pros and cons)\n",
    "\n",
    "Example of a website without an API: https://sociology.arizona.edu/faculty\n",
    "\n",
    "Example of a website with an API that is easy and OK to scrape directly: wikipedia\n",
    "\n",
    "**To scrape a website directly, you need to:**\n",
    "\n",
    "* Learn how to read and understand the website's raw code (every website is made up of a mix of HTML, CSS, and Javascript -- HTML is the most important for scraping)\n",
    "* Use scraping libraries: we learn `BeautifulSoup` and `requests` (the latter is used for both types of websites, with and without an API)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkblue'> <h2> 2. Scraping in Python: Overview of `requests` and `beautifulsoup` </h2> </font>\n",
    "\n",
    "We are going to use `requests` and `beautiful soup`. If you haven't installed them on your machine, please do so before loading them (see Canvas for details; I assume you use Python with Anaconda in this course).\n",
    "\n",
    "* <b>requests</b>: library to interact with web pages and get data from them. It sends HTTP requests to web servers and allows us (humans) to access the response. For more info see the official documentation: https://requests.readthedocs.io/en/master/\n",
    "\n",
    "* <b>beautiful soup (bs4)</b>: library to parse and extract the data. It allows us to navigate and extract data (i.e. the desired tags) from the HTML and other markup languages. See https://www.crummy.com/software/BeautifulSoup/ You need this library only for scraping directly a website.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests                       # to interact with websites and request/get data from them \n",
    "from bs4 import BeautifulSoup as bs   # to parse and extract data from websites \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The library <code>requests</code> helps us making requests and getting data back ([documentation](https://requests.readthedocs.io/en/latest/)):**\n",
    "\n",
    "To understand what `requests` does, and web scraping more generally, we need to start with our daily use of the internet: when we open a website in a browser (e.g. Google) our machine sends a request to the website's server, asking for the information on that page. The server responds by sending to us the requested data in raw format (mainly written in HTML), which is then nicely displayed in our browser.\n",
    "\n",
    "Under the hood, the process looks something [like this](https://www.linkedin.com/pulse/what-happens-when-you-enter-url-browser-he-asked-victor-ohachor):\n",
    "\n",
    "* Computers talk to each other on the web by sending and receiving (GET) <b>data requests</b> and (POST) <b>data responses</b>: some making requests, some receiving and answering them, some doing both. \n",
    "\n",
    "* Every computer has an address that other computers can use or refer to. When you click on a page, the <b>web browser</b> of your computer (e.g., Chrome, Safari, etc.) makes a data request to the <b>web server</b> of that page (think at a database where all the info about that page are stored) and gets back a response object. \n",
    "\n",
    "**For example:**\n",
    "\n",
    "If you type https://macss.uchicago.edu/current-student-resources into your web browser and hit enter, these steps occurs under the hood:\n",
    "* your web browser translates what you typed into a <b>HTTP request</b> to tell the macss web server that you would like to access the info stored at <code>/current-student-resources</code> using the <code>https</code> protocol\n",
    "* the web server that hosts macss receives your request and sends back to your web browser a <b>HTTP response</b> code and response content (a bunch of files written in HTML)\n",
    "* your browser receives and <b>transforms this response content into a nice visual display</b> that might include texts, graphics, hyperlinks, etc.\n",
    "\n",
    "**For web scraping, we do not want to display the data, we want to collect them:** \n",
    "\n",
    "* So we use the library `requests` to send data requests, and get back a response; then we use the library `beautifulsoup` to parse extract these data \n",
    "* There are other Python libraries similar to `requests` (e.g. `urllib, urllib2, urllib3`); however, `requests` is the most widely used. The same applies to `beautifulsoup`: not the most powerful scraping library, but the most common and the first tool new scrapers use before moving on to more advanced libraries (e.g., `scrapy` or tools for dynamic scraping).\n",
    "\n",
    "Let's see how to use these libraries with our first example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkblue'> <h2> 3. Example 1: Chicago Wikipedia </h2> </font>\n",
    "\n",
    "Our first task is to scrape a table from this Chicago's Wikipedia page: https://en.wikipedia.org/wiki/Chicago\n",
    "* We first use <code>requests</code> to interact with this URL and store the data response we get back.\n",
    "* We then rely on <code>BeautifulSoup</code> to parse the response and extract the data. \n",
    "\n",
    "\n",
    "<font color='darkblue'> <h3> Requests </h3> </font>\n",
    "\n",
    "Let's start from the library `requests`. To start the process, we send a <code>get()</code> request to the URL and store the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_wiki = 'https://en.wikipedia.org/wiki/Chicago'\n",
    "response = requests.get(chicago_wiki)\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the minimum necessary code to use the library, but we can do more: set up a User-Agent, check our response status code, and verify the encoding.\n",
    "\n",
    "<h4> User-Agent </h4>\n",
    "\n",
    "You can specify your User-Agent by passing it in a dictionary to the `headers` parameter of `request.get`. More info [here](https://requests.readthedocs.io/en/latest/user/quickstart/#custom-headers). \n",
    "\n",
    "This is not required in general, and not needed for PA1 (do not add it there), but it is helpful for your own scraping project if you want make your intentions clear.\n",
    "\n",
    "For example, we can specify the goal of our scraper and provide a way to be contacted by the website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = { \"User-Agent\" : \"demo scraper for teaching purposes yourname@uchicago.edu\" } \n",
    "response = requests.get(chicago_wiki, headers = header)\n",
    "\n",
    "print(\"Our response code is:\", response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember computers make and send requests (see above)? \n",
    "\n",
    "OK, a User-Agent is a text string that your computer's web browser sends every time you make a request to a website web server. It communicates info about your device type, operating system, etc. This info is useful for the web server that receives it. Sometimes, setting a custom User-Agent might prevent you from getting blocked. To know your User-Agent, type \"what is my user agent\" in your Google search bar. A Chrome User-Agent on Windows looks similar to this:\n",
    "\n",
    "    <code>user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.183 Safari/537.36\"</code>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Response Status Code </h4>\n",
    "\n",
    "Another useful task when you use the `requests` library is checking the response status code: 2xx codes bring good news, 4xx or 5xx codes mean errors. There are [several status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes), these are the most common: \n",
    "* \"200 OK\": standard response for successful HTTP requests\n",
    "* \"404 not found\": the page is not available\n",
    "* \"403 forbidden\": the server rejected the request\n",
    "* \"503 server unavailable\": usually a temporary issue, e.g. server overloaded or under maintenance\n",
    "* \"504 gateway timeout\": indicates a time issue (a response was not sent within the expected time frame)\n",
    "\n",
    "In our case, it is likely that our request will be successful, so we should get a 200 code, which means that the webpage has accepted the request. If you get an error (4xx or 5xx codes), it might mean several things. One of the most common is that the webpage is 404: since this comes from the website, there is nothing you can do on your side to \"fix it\". You might try later or manage the response error so your code does not break when it runs into a response code error (more on this in the next lectures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Our response code is:\", response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Encoding </h4>\n",
    "\n",
    "The request we send out is in bytes: the process called encoding translates bytes into letters/characters that humans can read. When the library `requests` gets the data back from the webpage, it encodes them automatically by making an educated guess as to what encoding scheme applies to a given language.\n",
    "\n",
    "The most common encoding is <code>UTF-8</code>, which is the default and works well for English and most languages, but not all. \n",
    "\n",
    "You may run into encoding issues with web scraping. This is because the representation of characters is not always consistent across the computers! In practice, this means you should check the default encoding, and you can leave unchanged, but if you notice issues along the way, go back and change it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Response default encoding is:\", response.encoding)\n",
    "\n",
    "# change encoding\n",
    "#response.encoding = 'latin-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Turn response into text </h4>\n",
    "\n",
    "OK, at this point we got our response object from the Chicago wikipedia server and we stored it into a \"response\" variable. \n",
    "\n",
    "Our \"response\" holds a reference to a request object; but we want to be able to read it, thus we turn it into a string with `.text` (if you have non-text content like images, use `.content` instead):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_txt = response.text\n",
    "print(type(response_txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkblue'> <h3>  Website Structure </h3> </font>\n",
    "\n",
    "Now we have the HTML of the Chicago Wikipedia page stored as a text in our <code>response_txt</code> object. We are going to use BeautifulSoup (or \"bs\" for short) to parse it, but, remember, bs won't do the hard job of understanding a webpage structure and telling us what to extract!  \n",
    "\n",
    "It can only help us parsing a webpage and extracting the data *once we know where to find it (e.g. which tag to grab)*. Therefore, to use bs for scraping task we need to learn the basic elements of a webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A website is made of the following elements:\n",
    "\n",
    "* <b>HTML</b> which means HyperText Markup Language and is the core element of a website. HTML uses a set of tags to organize the webpage (i.e., makes the text bold, creates body text, paragraphs, inserts hyperlinks, etc.), but when the page is displayed the markup language is hidden\n",
    "\n",
    "* <b>CSS</b> which means Cascading Style Sheets, it adds styling to make the page looks nicer \n",
    "\n",
    "* <b>JS</b> Javascript code is used to add interactivity to the page, and you need \"dynamic web scraping\" techniques to interact with it\n",
    "\n",
    "* <b>Other stuff</b> for example images (jpg and png allow webpages to show pictures), hyperlinks, videos or multimedia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic structure of HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML is the most important language we need to learn for web scraping; makes the \"skeleton\" or structure of a website\n",
    "\n",
    "Messy to read, but it follows a [hierarchical-tree-like structure](https://www.researchgate.net/figure/HTML-source-code-represented-as-tree-structure_fig10_266611108) since it embeds tags within tags (everything marked with <> is a tag)\n",
    "\n",
    "Standard HTML syntax (simple example): ``<tagname> contents </tagname>`` \n",
    "\n",
    "```html \n",
    "   <html>\n",
    "     <head>\n",
    "        <title>general info about the page</title>\n",
    "     </head>\n",
    "     <body>\n",
    "       <p>a paragraph that holds some text about the page</p>\n",
    "       <p>another paragraph which might contain <strong>additional</strong> markup</p>\n",
    "       <p>...</p>\n",
    "     </body>\n",
    "   </html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tags\n",
    "\n",
    "In web scraping, tags are fundamental because we collect information from webpages using them:\n",
    "\n",
    "* tags are organized in a tree-like structure and are nested within each other\n",
    "* tags go in pairs: one on each end of the content that they include, there is a \"start tag\" and an \"end tag\" (with a slash), for example `<p>hello</p>`\n",
    "* tags can have attributes (id and class attributes are the most useful for scraping)\n",
    "\n",
    "There are <b>several tags</b>, for example:\n",
    "\n",
    "* section headings: `<h1>...</h1>` to `<h6>...</h6>`\n",
    "represent six levels of section headings, `<h1>` is the highest section level and `<h6>` is the lowest\n",
    "\n",
    "\n",
    "* body:  ``<body>...</body>``\n",
    "contains the text and markup that is to be displayed, things like: text formatting (e.g. bold, italic, etc.), tables, paragraphs, lists, hyperlinks. Each will have its own tag inside the main body tag\n",
    "\n",
    "\n",
    "* links: \n",
    " - ``<a href=\"http://college.uchicago.edu\">The College</a>``\n",
    " -  Note that `a` is the tag and `href` is the tag attribute and `\"http://...\"` is the attribute's value. Links are web page requests embedded in another web page and are fundamental to the whole \"browsing\" experience. The idea is that, as you are reading some page, you can click on a hyperlink and be taken to other pages\n",
    "\n",
    "\n",
    "* images: ``<img style=\"height: 120px;\" alt=\"\" src=\"images/freelunch.png\">``\n",
    "\n",
    "\n",
    "* paragraph:\n",
    "  -  ``<p> … some text … </p>``\n",
    "  -  ``<p class=\"courseblocktitle”> course title… </p>``\n",
    "\n",
    "\n",
    "* comment: ``<!-- ... -->``\n",
    "\n",
    "\n",
    "* division or section:\n",
    "  - ``<div> … </div>``\n",
    "  - ``<div class=\"courseblock main\">…stuff...</div>``\n",
    "\n",
    "\n",
    "* table:\n",
    "```html\n",
    "     <table>\n",
    "       <tr>\n",
    "          <th>...</th>\n",
    "             ...\n",
    "          <th>...</th>\n",
    "       </tr>\n",
    "       <tr>\n",
    "          <td>...</td>\n",
    "             ...\n",
    "          <td>...</td>\n",
    "       </tr>\n",
    "     </table>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few more pieces of information to remember about tags:\n",
    "    \n",
    "1. Tags have commonly used names that depend on their positions in relations to other tags: \n",
    "    * <b>child</b>: the tag inside another tag (e.g. the `p`tag is usually a child of the `body` tag)\n",
    "    * <b>parent</b>: the tag that contains another tag\n",
    "    * <b>sibling</b>: two tags are siblings if they are nested inside the same parent\n",
    "\n",
    "\n",
    "2. <b>Class</b> and <b>id</b> are special attributes that specify more information about a given tag, usually a certain CSS style:\n",
    "* not all tags have a class or id attribute\n",
    "* the same class can be shared between elements but each element can only have one id. \n",
    "* in web scraping, class and id attributes are important because they offer details about a tag and so they help us locating it\n",
    "\n",
    "3. See [this list](https://developer.mozilla.org/en-US/docs/Web/HTML/Element) for a complete list of tags and their meaning for your scraping projects!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Back to our example to observe these things in practice:</b>\n",
    "\n",
    "Safari:\n",
    "* ensure the developer menu is enabled: open Safari > Settings > Advanced > Check the \"Show Develop menu in menu bar\" checkbox\n",
    "* go to https://en.wikipedia.org/wiki/Chicago, right click on it and select \"Inspect Element\"\n",
    "* on the search bar, there should be small target that you can use to select tags\n",
    "\n",
    "Chrome: \n",
    "* go to https://en.wikipedia.org/wiki/Chicago, right click on it and select \"Inspect\" (can also click on the \"View page source\") \n",
    "* there should be a small box with an arrow icon that you can use to select tags \n",
    "\n",
    "Browse the website and notice the elements we learned (e.g., tree-like structure; tags are nested and go in pairs, etc.)\n",
    "\n",
    "<b>Important: each webpage is different!</b> Meaning its HTML structure and tags cannot be determined in advance. This requires some general knowledge of HTML, and time and patience to identify which tags to use to scrape the data we want. In an ideal world, webpages are well made (in that they rely on well-designed and clear HTML structure) but in reality, this is often not the case!\n",
    "\n",
    "\n",
    "<font color='darkblue'> <h3> Beautiful Soup </h3> </font>\n",
    "\n",
    "<h4> Store and parse the HTML </h4>\n",
    "\n",
    "Now we use Beautiful Soup to get this webpage into Python and start scraping!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use bs to parse the content of our response_txt variable \n",
    "\n",
    "soup = bs(response_txt, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the `html.parser` (other common parsers `html5lib` and `lxml`). See [here]( https://www.crummy.com/software/BeautifulSoup/bs4/doc/#differences-between-parsers) for more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Common Beautiful Soup Methods </h4>\n",
    "\n",
    "Now that we have saved the entire HTML or \"skeleton\" of the webpage into an object (that we called \"soup\"), we want to extract information from our \"soup\". The most important BeautifulSoup methods to do so are `find()` and `find_all()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find() returns the FIRST instance of a tag (examples of tags: p, table, td, img, title, etc.)\n",
    "\n",
    "print(soup.find(\"title\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine find() with get_text() to extract text from a specific tag\n",
    "\n",
    "print(soup.find('title').get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same thing using the attribute notation: to access tags as if they were attributes of the object\n",
    "print(soup.title)\n",
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: the attribute or dot notaton is nice, but is more limited than the explicit notation. It is OK to use for elements that only appear ONCE per page, because it only gives back the first element! It is always safer to be explicit: use `.find()` for retrieving single elements and `.find_all()` for multiple elements. \n",
    "\n",
    "For instance, to find all paragraph tags, we can use `find_all()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_all() returns ALL instances of a tag on a given web page\n",
    "\n",
    "len(soup.find_all(\"p\"))\n",
    "print(soup.find_all(\"p\"))\n",
    "\n",
    "# compare with soup.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to extract the text from each of these <p> tags at once, this won't work... why?\n",
    "#soup.find_all('p').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to loop over each tag\n",
    "\n",
    "paragraphs = soup.find_all('p')\n",
    "\n",
    "for par in paragraphs:\n",
    "    print(par.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can save all <p> tags into a list \n",
    "\n",
    "paragraphs = soup.find_all('p')\n",
    "\n",
    "par_list = []\n",
    "for par in paragraphs:\n",
    "    par_txt = par.get_text()\n",
    "    par_list.append(par_txt)\n",
    "print(par_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or you save all <p> tags into a dictionary\n",
    "# keys are the consecutive numbers of <p>, and values are the <p> text\n",
    "\n",
    "paragraphs = soup.find_all('p')\n",
    "\n",
    "par_dict = {}\n",
    "for index, par in enumerate(paragraphs):\n",
    "    par_txt = par.get_text()\n",
    "    par_dict[index] = par_txt   \n",
    "print(par_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access value using keys (e.g. access paragraph number 10)\n",
    "par_dict[10]\n",
    "\n",
    "# return all keys and all values each as list, or all pairs as list\n",
    "#list(par_dict.keys())\n",
    "#list(par_dict.values())\n",
    "#list(par_dict.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than scraping all tags of one type (like all `<p>` tags) we can also scrape all instances of an EXACT MATCH (like a specific `p` tag). For example, if we had a tag that looks like this:\n",
    "\n",
    "`<span class=\"mw-headline\" id=\"Etymology_and_nicknames\">Etymology and nicknames</span>`\n",
    "\n",
    "We could write code like this to extract it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syntax 1\n",
    "#print(soup.find_all(\"span\", {\"class\": \"mw-headline\"}))\n",
    "\n",
    "# syntax 2\n",
    "#print(soup.find_all(\"span\", class_= \"mw-headline\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Scraping Tables </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Chicago wiki page](https://en.wikipedia.org/wiki/Chicago) has a few tables in it. Let's say that we want to scrape the \"Major league professional teams in Chicago (ranked by attendance)\" table in the page. And turn it into a DataFrame object. \n",
    "\n",
    "First, take a look at the page by \"inspecting\" it (see above for instructions). We want to look for the tag and its class attribute and put them into our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does not work although is the tag we see on the website by inspecting it\n",
    "#soup.find_all(\"table\", class_= \"wikitable sortable jquery-tablesorter\")\n",
    "\n",
    "# this works\n",
    "soup.find_all(\"table\", class_= \"wikitable sortable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The reason the first code doesn't work is in the `soup` variable. The `find_all()` method searches the content stored in that variable (vs. the webpage we see when we \"inspect\" elements). To confirm, try typing `print(soup)` or `print(soup.prettify())` and then search for the table tag within the output.\n",
    "\n",
    "Issues like this are common, and you will often need to adjust your code. Here are some debugging tips:\n",
    "\n",
    "* Check the content of your soup variable to ensure it includes the elements you are trying to scrape.\n",
    "* Experiment with different parsers. For example, we used `html.parser` here, but other options are available (a list of common parsers is linked in earlier sections of this notebook).\n",
    "* Check  if the website has JavaScript-rendered components (most common scenario): BeautifulSoup cannot handle JavaScript directly and may only partially scrape or not scrape at all such content "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can grab it and print how it looks like in beautiful soup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_table = soup.find(\"table\", class_= \"wikitable sortable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also render this table in our Jupyter notebook, so that we can better see what we are working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(str(sports_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, we have not turn the table into a dataframe yet, we are just displaying it inside Jupyter to help inspecting it and grabbing the tags we need (you can do it directly from the page).\n",
    "\n",
    "It looks like each row of data is between `<tr>` tags, for \"table row.\" We can pull out the raw text within each one of these rows using the `text` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with a loop\n",
    "rows = []\n",
    "for i in (sports_table.find_all(\"tr\")):\n",
    "    rows.append(i.text)\n",
    "print(rows)\n",
    "len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with a list comprehension = [expression for item in iterable if condition]\n",
    "rows = [i.text for i in sports_table.find_all(\"tr\")]\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still leaves us with a bunch of new line characters `\\n` (some data cleaning is very common in scraping!). We can deal with them using Python's built-in string methods lie `strip()` and `split()`. \n",
    "\n",
    "Note that each column entry is delineated by two new line characters and each row starts with one new line character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip new line characters from start and end, split on double new line\n",
    "rows_clean = [i.text.strip('\\n').split('\\n\\n') for i in sports_table.find_all(\"tr\")]\n",
    "rows_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"list of lists\" is something that we can work with and easily bring into a [Pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data = rows_clean[1:], columns = rows_clean[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still a few data cleaning tasks that we could complete if we wanted to analyze this data (i.e. change the Attendance column to an integer data type, we will talk more about cleaning tasks in the weeks ahead), but we have a scraped table as a df!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkblue'> <h3> Exercise 1 </h3> </font>\n",
    "\n",
    "**Your task: use the code above as example to scrape the \"Racial Composition\" table from [Chicago wiki page](https://en.wikipedia.org/wiki/Chicago). Then use regular expressions to turn clean the scraped data and turn it into a pandas df. Do not use AI.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code to request data\n",
    "chicago_wiki = 'https://en.wikipedia.org/wiki/Chicago'\n",
    "response = requests.get(chicago_wiki)\n",
    "response_txt = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code to parse (e.g., \"soup\") and scrape raw data\n",
    "soup = bs(response_txt, \"html.parser\") # or bs(response.text, \"html.parser\") if you skip line above\n",
    "race_table = soup.find(\"table\", class_= \"wikitable sortable collapsible\")\n",
    "rows = [i.text for i in race_table.find_all(\"tr\")]\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code to clean data using regular expressions \n",
    "\n",
    "import re\n",
    "\n",
    "cleaned_rows = []\n",
    "for row in rows:\n",
    "    row = row.strip('\\n')\n",
    "    row = re.sub('\\n\\n', '\\n', row)\n",
    "    row = re.sub('\\[\\w+\\]', '', row)\n",
    "    row = row.split('\\n')\n",
    "    cleaned_rows.append(row)\n",
    "cleaned_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code to turn the collected data into a pandas df\n",
    "df = pd.DataFrame(data = cleaned_rows[1:], columns = cleaned_rows[0])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkblue'> <h2> 4. Example 2: Extract Faculty information </h2> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use another website to practice what we learned so far and illustrate another common scraping task: <b>collecting contact information</b>. You can apply the same logic to get contact information from any website (companies, NGOs, congress members, other univerisities, etc.)\n",
    "\n",
    "Our tasks:\n",
    "1. Make a request to https://sociology.arizona.edu/faculty and save it into a response object\n",
    "1. Turn the response object into text and parse it\n",
    "1. Identify which HTML tags to use to scrape the following info from each faculty member: names, emails, titles\n",
    "1. Put it all togheter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 1: make a request with the request library\n",
    "\n",
    "url = \"https://sociology.arizona.edu/faculty\"\n",
    "response = requests.get(url)\n",
    "print(\"Our response code is:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2: turn the response into text and parse it with the bs library\n",
    "\n",
    "soup = bs(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips to extract names, emails, etc.:\n",
    "\n",
    "* Go to the url you want to scrape (https://sociology.arizona.edu/faculty)\n",
    "* Use the \"Inspect\" option in the development tools to inspect the HTML (see above for more)\n",
    "* You should discover that all data (name, email, phone, etc.) for each faculty are nested under a tag called <code>div class=\"az-person-row\"</code>: <code>div</code> is the actual tag (a division or section tag), the rest is its specific class attribute\n",
    "* Spend some time exploring all tags nested under this <code>div</code> tag to find the info you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is one way to set up your scraper (e.g., using the higher-level tag and attribute `div class=\"az-person-row\"`). However, there are other functional approaches to scrape data from this website. Select the approach that best fits your specific needs and the structure of the website's HTML.\n",
    "\n",
    "**Beyond this specific example, your scraping code should always include a way to handle potential missing data for each element you scrape (see the example provided in TASK 4 below)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 3.1: NAMES are stored under a <span> tag \n",
    "\n",
    "names = []\n",
    "\n",
    "for row in soup.find_all('div',  attrs = {'class': 'az-person-row'}): \n",
    "    name = row.find('span', attrs = {'class': 'field field--name-title field--type-string field--label-hidden'}).text\n",
    "    names.append(name)\n",
    "\n",
    "print(\"\\n\", \"Names of faculty members:\", \"\\n\", names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 3.2: EMAILS are stored as text under <a href>, nested under <div> tag...\n",
    "\n",
    "emails = []\n",
    "\n",
    "for row in soup.find_all('div',  attrs = {'class': 'az-person-row'}): \n",
    "    div_tag = row.find('div', attrs = {'class': 'field field--name-field-az-email field--type-email field--label-hidden text-truncate d-block'})\n",
    "    #print(div_tag)\n",
    "    if div_tag is not None:\n",
    "        email = div_tag.find(\"a\", href = True).text  # only \"a\" also works here\n",
    "    #print(email)\n",
    "    emails.append(email) \n",
    "\n",
    "print(emails)\n",
    "\n",
    "# note: see TASK 4 below for a better way of setting this code up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 3.3: TITLES are stored as text under two nested div tags\n",
    "\n",
    "titles = []\n",
    "  \n",
    "for row in soup.find_all('div',  attrs = {'class': 'az-person-row'}): \n",
    "    title = row.find('div', attrs = {'class': 'field field--name-field-az-titles field--type-string field--label-hidden field__items'}).text\n",
    "    title = title.replace(\"\\n\", \" \").strip()\n",
    "    titles.append(title) \n",
    "\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 4: Combine everything in one piece of code\n",
    "# Collect names, emails, and titles in one loop while checking for missing data\n",
    "\n",
    "contacts = []\n",
    "\n",
    "for row in soup.find_all('div',  attrs = {'class': 'az-person-row'}): \n",
    "    \n",
    "    # scrape names\n",
    "    name_tag = row.find('span', attrs = {'class': 'field field--name-title field--type-string field--label-hidden'})\n",
    "    if name_tag:\n",
    "        name = name_tag.text\n",
    "    else:\n",
    "        name = \"NA\"\n",
    "    #name = name_tag.text if name_tag else \"NA\"\n",
    "      \n",
    "    # scrape emails\n",
    "    email_tag = row.find('div', attrs = {'class': 'field field--name-field-az-email field--type-email ' \\\n",
    "                                               'field--label-hidden text-truncate d-block'})\n",
    "    if email_tag:\n",
    "        email = email_tag.text\n",
    "    else:\n",
    "        email = \"NA\"\n",
    "    #email = email_tag.text.strip() if email_tag else \"NA\"\n",
    "        \n",
    "    # scrape titles\n",
    "    title_tag = row.find('div', attrs = {'class': 'field field--name-field-az-titles field--type-string ' \\\n",
    "                                               'field--label-hidden field__items'})\n",
    "    if title_tag: \n",
    "        title = title_tag.text.replace(\"\\n\", \" \").strip()\n",
    "    else: \n",
    "        title = \"NA\"\n",
    "    #title = title_tag.text.replace(\"\\n\", \" \").strip() if title_tag else \"NA\"\n",
    "    \n",
    "    contacts.append([name, email, title])  \n",
    "\n",
    "# print results line by line and sort them\n",
    "for row in sorted(contacts):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results in a pandas dataframe \n",
    "\n",
    "df = pd.DataFrame(contacts)\n",
    "display(df) # same as print(df) but nicely formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename df and its columns\n",
    "\n",
    "faculty = df.rename (columns = {0: 'name', 1: 'email', 2: 'title'})\n",
    "display (faculty[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the collected data as csv using the \"DataFrame.to_csv\" function\n",
    "# change the path to store the results on your machine\n",
    "\n",
    "faculty.to_csv (r'\\Users\\Sabrina Nardin\\Desktop\\faculty.csv', encoding = 'utf-8', index = False)\n",
    "print('Data exported!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: passing the path as raw string with `r` works for Windows but not for Macs: `r` allows you to treat escape sequences as characters, but path specifications for Macs use a '/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkblue'> <h3> Exercise 2 </h3> </font>\n",
    "\n",
    "**Your task: Scrape all faculty names and all faculty phone numbers from https://sociology.arizona.edu/faculty. Note that a few faculty members do not have a phone number, your scraper must account for the missing phone numbers. Write the same code using a list to store names and phones, and using a dictionary to do the same. Do not use AI.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to scrape names and phone using a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to scrape names and phones using a dictionary\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
